{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"attention_from_scratch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMI+9xNcFTZJXhGzQRq4kxK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"kaaIjbslzq6q","executionInfo":{"status":"ok","timestamp":1656025471044,"user_tz":240,"elapsed":149,"user":{"displayName":"Juan D","userId":"01890547855168746202"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"code","source":["class SelfAttention(nn.Module):\n","  def __init__(self,embed_size,heads):\n","    super(SelfAttention,self).__init__()\n","    self.embed_size=embed_size\n","    self.heads=heads\n","    self.head_dim=embed_size//heads\n","\n","    assert(self.head_dim*heads == embed_size), \"Embed_size needs to be divisible by heads\"\n","\n","    self.values = nn.Linear(self.head_dim,self.head_dim,bias=False)\n","    self.keys = nn.Linear(self.head_dim,self.head_dim,bias=False)\n","    self.queries = nn.Linear(self.head_dim,self.head_dim,bias=False)\n","    self.fc_out = nn.Linear(heads*self.head_dim,embed_size)\n","\n","    def forward(self,values,keys,query,mask):\n","      N = query.shape[0]\n","      value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n","\n","      #Split embeddings into self.heads pieces\n","      values= values.reshape(N,value_len,self.heads,self.head_dim)\n","      keys= keys.reshape(N,key_len,self.heads,self.head_dim)\n","      queries= query.reshape(N,query_len,self.heads,self.head_dim)\n","      \n","      values = self.values(values)\n","      keys = self.keys(keys)\n","      queries = self.queries(queries)\n","      \n","      energy = torch.einsum(\"nqhd,nkhd->nhqk\",[queries,keys])\n","      #query shape: (N,query_len,heads,heads_dim)\n","      #keys shape: (N,key_len,heads,heads_dim)\n","      #energy shape: (N, heads, query_len, key_len)\n","\n","      if mask is not None:\n","        energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n","      \n","      attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n","\n","      out = torch.einsum(\"nhql,nlhd->nqhd\", [attention,values]).reshape(\n","          N,query_len,self.heads*self.head_dim\n","      )\n","      #attention shape: (N, heads, query_len,keys_len)\n","      #values shape: (N, value_len,heads, head_dim)\n","      # (N,query_len,heads,head_dim)\n","      # then flatten last two dimensions\n","\n","      out=self.fc_out(out)\n","      return out\n","\n","class TransformerBlock(nn.Module):\n","  def __init__(self,embed_size,heads,dropout,forward_expansion):\n","    super(TransformerBlock,self).__init__()\n","    self.attention=SelfAttention(embed_size,heads)\n","    self.norm1 =  nn.LayerNorm(embed_size)\n","    self.norm2 =  nn.LayerNorm(embed_size)\n","\n","    self.feed_forward = nn.Sequential(\n","        nn.Linear(embed_size, forward_expansion*embed_size),\n","        nn.ReLU(),\n","        nn.linear(forward_expansion*embed_size, embed_size)\n","    )\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self,value,key,query,mask):\n","    attention = self.attention(value,key,query,mask)\n","    x = self.dropout(self.norm1(attention+query))\n","    forward = self.feed_forward(x)\n","    out = self.dropout(self.norm2(forward+x))\n","    return out\n","\n","class Encoder(nn.Module):\n","  def __init__(\n","    self,\n","    src_vocab_size,\n","    embed_size,\n","    num_layers,\n","    heads,\n","    device,\n","    forward_expansion, \n","    dropout,\n","    max_length,\n","  ):\n","    super(Encoder,self).__init__()\n","    self.embed_size = embed_size\n","    self.device = device\n","    self.word_embedding = nn.Embedding(src_vocab_size,embed_size)\n","    self.position_embeding = nn.Embedding(max_length,embed_size)\n","\n","    self.layers = nn.ModuleList(\n","        [\n","         TransformerBlock(\n","             embed_size,\n","             heads,\n","             dropout=dropout\n","             forward_expansion=forward_expansion\n","         )  \n","         for _ in range(num_layers)]\n","    )\n","    self.dropout=nn.Dropout(dropout)\n","\n","  def forward(self,x,mask):\n","    N,seq_length = x.shape\n","    positions = torch.arange(0,seq_length).expand(N,seq_length).to(self.device)\n","\n","    out = seldf.dropout(self.word_embedding(x)+self.position_embeding(positions))\n","\n","    for layer in self.layers:\n","      out = layer(out,out,out,mask)\n","    return out\n","\n","class DecoderBlock(nn.Module):\n","  def __init__(self,embed_size,heads,forward_expansion,dropout,device):\n","    super(DecoderBlock,self).__init__()\n","    self.attention = SelfAttention(embed_size,heads)\n","    self.norm = nn.LayerNorm(embed_size)\n","    self.TransformerBlock = TransformerBlock(\n","        embed_size,heads,dropout,forward_expansion\n","    )\n","    self.dropout=nn.Dropout(dropout)\n","\n","    def forward(self,x,value,key,src_mask,trg_mask):\n","      attention = self.attention(x,x,x,trg_mask)\n","      query = self.dropout(self.norm(attention+x))\n","      out = self.transformer_block(value,key,query,src_mask)\n","      return out\n","\n","class Decoder(nn.Module):\n","  def __init__(self,\n","               trg_vocab_size,\n","               embed_size,\n","               num_layers,\n","               heads,\n","               forward_expansion,\n","               dropout,\n","               device,\n","               max_length,\n","  ):\n","    super(Decoder,self).__init__()\n","    self.device = device\n","    self.word_embedding = nn.Embedding(trg_vocab_size,embed_size)\n","    self.position_embedding = nn.Embedding(max_length,embed_size)\n","    self.layers = nn.ModuleList(\n","        [DecoderBlock(embed_size,heads,forward_expansion,dropout,device)\n","        for _ in range(num_layers)]\n","    )\n","    self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n","    self.dropout = nn.Dropout(dropout)\n","  \n","  def forward(self,x,enc_out,src_mask,trg_mask):\n","    N,seq_length =  x.shape\n","    positions = torch.arange(0,seq_length).expand(N,seq_length).to(self.device)\n","    x = self.dropout(self.word_embedding(x)+self.position_embedding(positions))\n","\n","    for layer in self.layers:\n","      x = layer(x,enc_out,enc_out,src_mask,trg_mask)\n","\n","    out = self.fc_out(x)\n","    return out\n","\n","class Transformer(nn.Module):\n","  def __init__(\n","          self,\n","          src_vocab_size,\n","          trg_vocab_size,\n","          src_pad_idx,\n","          trg_pad_idx,\n","          embed_size=256,\n","          num_layers=6,\n","          forward_expansion=4,\n","          heads=8,\n","          dropout=0,\n","          device=\"cuda\",\n","          max_length=100,\n","  ):\n","\n","    super(Transformer,self).__init__()\n","\n","    self.encoder = Encoder(\n","        src_vocab_size,\n","        embed_size,\n","        num_layers,\n","        heads,\n","        device,\n","        forward_expansion,\n","        dropout,\n","        max_length\n","    )\n","\n","    self.decoder = Decoder(\n","        trg_vocab_size,\n","        embed_size,\n","        num_layers,\n","        heads,\n","        forward_expansion,\n","        dropout,\n","        device,\n","        max_length\n","    )\n","\n","    self.src_pad_idx = src_pad_idx\n","    self.trg_pad_idx = trg_pad_idx\n","    self.device = device\n","\n","  def make_src_mask(self,src):\n","    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","    # (N,1,1,src_len)\n","    return src_mask.to(self.device)\n","\n","  def make_trg_mask(self,trg):\n","    N,trg_len = trg.shape\n","    trg_mask =  torch.tril(torch.ones((trg_len,trg_len))).expand(\n","        N, 1, trg_len, trg_len\n","    )\n","    return  trg_mask.to(self.device)\n","  \n","  def forward(self,src,trg):\n","    src_mask = self.make_src_mask(src)\n","    trg_mask = self.make_trg_mask(trg)\n","    enc_src = self.encoder(src,src_mask)\n","    out = self.decoder(trg,enc_src,src_mask, trg_mask)\n","    return out "],"metadata":{"id":"ydZUtyuIzxRL","executionInfo":{"status":"ok","timestamp":1656025902287,"user_tz":240,"elapsed":136,"user":{"displayName":"Juan D","userId":"01890547855168746202"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"nuI7an8l3iiR"}},{"cell_type":"code","source":[""],"metadata":{"id":"jg81f6I01er3"},"execution_count":null,"outputs":[]}]}